# Evaluate PII Inference
dataset_args:
  dataset_path: ../src/pii_leakage/extern/echr
  dataset_mode: undefended
  sample_duplication_rate: 1

attack_args:
  attack_name: perplexity_reconstruction
  sampling_rate: 100
  seq_len: 128

sampling_args:
  top_k: 0.0
  top_p: 0.7
  typical_p: 1.0
  temperature: 0.8
  repetition_penalty: 1.0
  
eval_args:
  num_candidates: 64

model_args:
  architecture: gpt2-large
  pre_trained: True   # Start from a pre-trained checkpoint
  model_ckpt: ../examples/models/echr-gpt2-large # Edit this path to try your own model.
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
  # LoRA arguments
  # LoRA arguments
  load_in_4bit: False
  use_peft: False

ner_args:
  ner: flair
  ner_model: flair/ner-english-ontonotes-large
  anon_token: <MASK>
  anonymize: False
  tag_n_batches: 10_000
